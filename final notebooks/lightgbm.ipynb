{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Light Gradient Boosting with Cross Validation and Custom Loss Functions\n",
    "## Preliminaries\n",
    "The LightGBM implementation relies on [this](https://www.kaggle.com/ragnar123/simple-lgbm-groupkfold-cv) Kaggle notebook where the reduced data is taken from too.\n",
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "from typing import Union\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn import preprocessing\n",
    "import gc\n",
    "import lightgbm as lgb\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn import metrics\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Load Data and Reduce Memory Usage\n",
    "`reduce_mem_usage` appears in several notebooks, a possible first implementation could be https://www.kaggle.com/kyakovlev/m5-simple-fe. It transforms column types to the smallest type possible without losing information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "`read_data` reads the data from a pickle file and transforms it. The pickle file used is available at https://www.kaggle.com/ragnar123/m5-reduce-data and is a reduced version of the original challenge data. The sample submission from the competition is read as well. \n",
    "\n",
    "`transform` fills the null values for features that are non-numeric and encodes categorical features with a numeric value. Finally, the memory usage is reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    # read data\n",
    "    data = pd.read_pickle('/kaggle/input/m5-reduce-data/data_small.pkl')\n",
    "    # fillna and label encode categorical features\n",
    "    data = transform(data)\n",
    "    # read submission\n",
    "    submission = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sample_submission.csv')\n",
    "    return data, submission\n",
    "\n",
    "def transform(data):\n",
    "    # replace null values with 'unknown'\n",
    "    nan_features = ['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n",
    "    for feature in nan_features:\n",
    "        data[feature].fillna('unknown', inplace = True)\n",
    "        \n",
    "    # convert labels to a numeric value\n",
    "    cat = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n",
    "    for feature in cat:\n",
    "        encoder = preprocessing.LabelEncoder()\n",
    "        data[feature] = encoder.fit_transform(data[feature])\n",
    "        \n",
    "    # reduce memory usage\n",
    "    data = reduce_mem_usage(data)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Create Features\n",
    "`simple_fe` creates simple features like lags, rolling means and dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_fe(data):\n",
    "    data_fe = data[['id', 'demand']]\n",
    "    \n",
    "    window = 28\n",
    "    periods = [7, 15, 30, 90]\n",
    "    group = data_fe.groupby('id')['demand']\n",
    "    \n",
    "    # most recent lag data\n",
    "    for period in periods:\n",
    "        data_fe['demand_rolling_mean_t' + str(period)] = group.transform(lambda x: x.shift(window).rolling(period).mean())\n",
    "        data_fe['demand_rolling_std_t' + str(period)] = group.transform(lambda x: x.shift(window).rolling(period).std())\n",
    "        \n",
    "    # reduce memory\n",
    "    data_fe = reduce_mem_usage(data_fe)\n",
    "    \n",
    "    # get time features\n",
    "    data['date'] = pd.to_datetime(data['date'])\n",
    "    time_features = ['year', 'month', 'quarter', 'week', 'day', 'dayofweek', 'dayofyear']\n",
    "    dtype = np.int16\n",
    "    for time_feature in time_features:\n",
    "        data[time_feature] = getattr(data['date'].dt, time_feature).astype(dtype)\n",
    "        \n",
    "    # concat lag and rolling features with main table\n",
    "    lag_rolling_features = [col for col in data_fe.columns if col not in ['id', 'demand']]\n",
    "    data = pd.concat([data, data_fe[lag_rolling_features]], axis = 1)\n",
    "    \n",
    "    del data_fe\n",
    "    gc.collect()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Create Loss Functions\n",
    "`custom_asymmetric_train` is the training loss that calculates the gradient and hessian of a custom asymmetric MSE.\n",
    "\n",
    "`custom_asymmetric_valid` is the evaulation metric that uses a custom MSE.\n",
    "\n",
    "Both are based on [this kernel](https://www.kaggle.com/ragnar123/asymmetric-loss-functions-lgbm). The author ran some experiments on how much penalty is effective and concluded that 1.15 is the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_asymmetric_train(y_pred, y_true):\n",
    "    y_true = y_true.get_label()\n",
    "    residual = (y_true - y_pred).astype(\"float\")\n",
    "    grad = np.where(residual < 0, -2 * residual, -2 * residual * 1.15)\n",
    "    hess = np.where(residual < 0, 2, 2 * 1.15)\n",
    "    return grad, hess\n",
    "\n",
    "def custom_asymmetric_valid(y_pred, y_true):\n",
    "    y_true = y_true.get_label()\n",
    "    residual = (y_true - y_pred).astype(\"float\")\n",
    "    loss = np.where(residual < 0, (residual ** 2) , (residual ** 2) * 1.15) \n",
    "    return \"custom_asymmetric_eval\", np.mean(loss), False"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Configuration of LightGBM\n",
    "`run_lgb` contains the lgbm model. Here the data is separated into training and test data, the parameters are defined like the learning rate and bagging. The k-fold cross validation is set to have 5 folds. In each fold the out-of-fold prediction and test data are predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lgb(data, features, cat_features):\n",
    "    # reset_index\n",
    "    data.reset_index(inplace = True, drop = True)\n",
    "    \n",
    "    # going to evaluate with the last 28 days\n",
    "    x_train = data[data['date'] <= '2016-04-24']\n",
    "    y_train = x_train['demand']\n",
    "    test = data[data['date'] >= '2016-04-25']\n",
    "\n",
    "    # define hyperparammeters\n",
    "    params = {\n",
    "     'boosting_type': 'gbdt',\n",
    "     'n_jobs': -1,\n",
    "     'seed': 42,\n",
    "     'learning_rate': 0.1,\n",
    "     'bagging_fraction': 0.85,\n",
    "     'bagging_freq': 1, \n",
    "     'colsample_bytree': 0.85,\n",
    "     'colsample_bynode': 0.85,\n",
    "     'min_data_per_leaf': 32,\n",
    "     'num_leaves': 511,\n",
    "     'lambda_l1': 0.5,\n",
    "     'lambda_l2': 0.5}\n",
    "\n",
    "    #out-of-fold prediction\n",
    "    oof = np.zeros(len(x_train))\n",
    "    preds = np.zeros(len(test))\n",
    "    \n",
    "    # GroupKFold by week, month to avoid leakage and overfitting\n",
    "    kf = GroupKFold(5)\n",
    "    # get subgroups for each week, year pair\n",
    "    group = x_train['week'].astype(str) + '_' + x_train['year'].astype(str)\n",
    "    for fold, (trn_idx, val_idx) in enumerate(kf.split(x_train, y_train, group)):\n",
    "        print(f'Training fold {fold + 1}')\n",
    "        train_set = lgb.Dataset(x_train.iloc[trn_idx][features], y_train.iloc[trn_idx], \n",
    "                                categorical_feature = cat_features)\n",
    "        val_set = lgb.Dataset(x_train.iloc[val_idx][features], y_train.iloc[val_idx], \n",
    "                              categorical_feature = cat_features)\n",
    "        \n",
    "        # train with our custom loss function and evaluation metric\n",
    "        model = lgb.train(params, train_set, num_boost_round = 10000, early_stopping_rounds = 50, \n",
    "                          valid_sets = [train_set, val_set], verbose_eval = 50, fobj = custom_asymmetric_train, \n",
    "                          feval = custom_asymmetric_valid)\n",
    "    \n",
    "        # predict oof\n",
    "        oof[val_idx] = model.predict(x_train.iloc[val_idx][features])\n",
    "\n",
    "        # predict test\n",
    "        preds += model.predict(test[features]) / 5\n",
    "        \n",
    "        print('-'*50)\n",
    "        print('\\n')\n",
    "        \n",
    "    oof_rmse = np.sqrt(metrics.mean_squared_error(y_train, oof))\n",
    "    print(f'Our out of folds rmse is {oof_rmse}')\n",
    "        \n",
    "    test = test[['id', 'date', 'demand']]\n",
    "    test['demand'] = preds\n",
    "    return test"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Submission File\n",
    "`predict` transforms the predictions into the correct format for the submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test, submission):\n",
    "    predictions = pd.pivot(test, index = 'id', columns = 'date', values = 'demand').reset_index()\n",
    "    predictions.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\n",
    "\n",
    "    evaluation_rows = [row for row in submission['id'] if 'evaluation' in row] \n",
    "    evaluation = submission[submission['id'].isin(evaluation_rows)]\n",
    "\n",
    "    validation = submission[['id']].merge(predictions, on = 'id')\n",
    "    final = pd.concat([validation, evaluation])\n",
    "    final.to_csv('submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Train and Evaluate\n",
    "`train_and_evaluate` is the main function that will run the entire program. 2 years plus the amount that is needed for the lags are used for training. First the data is read and then the features are added to it. The model is trained with 5 folds  anf finally the predictions are saved to a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading our data...\n",
      "Mem. usage decreased to 1540.89 Mb (57.1% reduction)\n",
      "We have 1011 days of training history\n",
      "we have 163 days left\n",
      "We have enough training data, lets continue\n",
      "Running simple feature engineering...\n",
      "Mem. usage decreased to 1027.26 Mb (58.5% reduction)\n",
      "Removing first 118 days\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training model with 28 features...\n",
      "Training fold 1\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's custom_asymmetric_eval: 4.65576\tvalid_1's custom_asymmetric_eval: 5.55098\n",
      "[100]\ttraining's custom_asymmetric_eval: 4.18923\tvalid_1's custom_asymmetric_eval: 5.38161\n",
      "[150]\ttraining's custom_asymmetric_eval: 3.95154\tvalid_1's custom_asymmetric_eval: 5.3326\n",
      "[200]\ttraining's custom_asymmetric_eval: 3.77857\tvalid_1's custom_asymmetric_eval: 5.29965\n",
      "[250]\ttraining's custom_asymmetric_eval: 3.64519\tvalid_1's custom_asymmetric_eval: 5.28178\n",
      "[300]\ttraining's custom_asymmetric_eval: 3.5301\tvalid_1's custom_asymmetric_eval: 5.25928\n",
      "[350]\ttraining's custom_asymmetric_eval: 3.43903\tvalid_1's custom_asymmetric_eval: 5.25082\n",
      "[400]\ttraining's custom_asymmetric_eval: 3.35606\tvalid_1's custom_asymmetric_eval: 5.24039\n",
      "[450]\ttraining's custom_asymmetric_eval: 3.28006\tvalid_1's custom_asymmetric_eval: 5.23087\n",
      "[500]\ttraining's custom_asymmetric_eval: 3.21491\tvalid_1's custom_asymmetric_eval: 5.22795\n",
      "[550]\ttraining's custom_asymmetric_eval: 3.15298\tvalid_1's custom_asymmetric_eval: 5.22154\n",
      "[600]\ttraining's custom_asymmetric_eval: 3.09826\tvalid_1's custom_asymmetric_eval: 5.21704\n",
      "[650]\ttraining's custom_asymmetric_eval: 3.04674\tvalid_1's custom_asymmetric_eval: 5.21644\n",
      "Early stopping, best iteration is:\n",
      "[616]\ttraining's custom_asymmetric_eval: 3.08101\tvalid_1's custom_asymmetric_eval: 5.216\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training fold 2\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's custom_asymmetric_eval: 4.62412\tvalid_1's custom_asymmetric_eval: 5.88675\n",
      "[100]\ttraining's custom_asymmetric_eval: 4.16155\tvalid_1's custom_asymmetric_eval: 5.6953\n",
      "[150]\ttraining's custom_asymmetric_eval: 3.92257\tvalid_1's custom_asymmetric_eval: 5.63025\n",
      "[200]\ttraining's custom_asymmetric_eval: 3.75724\tvalid_1's custom_asymmetric_eval: 5.58744\n",
      "[250]\ttraining's custom_asymmetric_eval: 3.63005\tvalid_1's custom_asymmetric_eval: 5.57172\n",
      "[300]\ttraining's custom_asymmetric_eval: 3.51982\tvalid_1's custom_asymmetric_eval: 5.55206\n",
      "[350]\ttraining's custom_asymmetric_eval: 3.42806\tvalid_1's custom_asymmetric_eval: 5.53779\n",
      "[400]\ttraining's custom_asymmetric_eval: 3.34378\tvalid_1's custom_asymmetric_eval: 5.52197\n",
      "[450]\ttraining's custom_asymmetric_eval: 3.26845\tvalid_1's custom_asymmetric_eval: 5.51431\n",
      "[500]\ttraining's custom_asymmetric_eval: 3.20293\tvalid_1's custom_asymmetric_eval: 5.51012\n",
      "[550]\ttraining's custom_asymmetric_eval: 3.1422\tvalid_1's custom_asymmetric_eval: 5.50142\n",
      "[600]\ttraining's custom_asymmetric_eval: 3.08702\tvalid_1's custom_asymmetric_eval: 5.49885\n",
      "[650]\ttraining's custom_asymmetric_eval: 3.03448\tvalid_1's custom_asymmetric_eval: 5.49291\n",
      "[700]\ttraining's custom_asymmetric_eval: 2.98637\tvalid_1's custom_asymmetric_eval: 5.48934\n",
      "[750]\ttraining's custom_asymmetric_eval: 2.94317\tvalid_1's custom_asymmetric_eval: 5.4887\n",
      "[800]\ttraining's custom_asymmetric_eval: 2.90154\tvalid_1's custom_asymmetric_eval: 5.486\n",
      "[850]\ttraining's custom_asymmetric_eval: 2.86046\tvalid_1's custom_asymmetric_eval: 5.48142\n",
      "[900]\ttraining's custom_asymmetric_eval: 2.82362\tvalid_1's custom_asymmetric_eval: 5.47871\n",
      "[950]\ttraining's custom_asymmetric_eval: 2.78742\tvalid_1's custom_asymmetric_eval: 5.47625\n",
      "[1000]\ttraining's custom_asymmetric_eval: 2.75221\tvalid_1's custom_asymmetric_eval: 5.4716\n",
      "[1050]\ttraining's custom_asymmetric_eval: 2.71893\tvalid_1's custom_asymmetric_eval: 5.46871\n",
      "Early stopping, best iteration is:\n",
      "[1046]\ttraining's custom_asymmetric_eval: 2.72138\tvalid_1's custom_asymmetric_eval: 5.46864\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training fold 3\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's custom_asymmetric_eval: 4.7426\tvalid_1's custom_asymmetric_eval: 5.24875\n",
      "[100]\ttraining's custom_asymmetric_eval: 4.26234\tvalid_1's custom_asymmetric_eval: 5.077\n",
      "[150]\ttraining's custom_asymmetric_eval: 4.01351\tvalid_1's custom_asymmetric_eval: 5.02263\n",
      "[200]\ttraining's custom_asymmetric_eval: 3.83666\tvalid_1's custom_asymmetric_eval: 4.98202\n",
      "[250]\ttraining's custom_asymmetric_eval: 3.70305\tvalid_1's custom_asymmetric_eval: 4.97016\n",
      "[300]\ttraining's custom_asymmetric_eval: 3.59047\tvalid_1's custom_asymmetric_eval: 4.96201\n",
      "[350]\ttraining's custom_asymmetric_eval: 3.49326\tvalid_1's custom_asymmetric_eval: 4.95528\n",
      "[400]\ttraining's custom_asymmetric_eval: 3.40686\tvalid_1's custom_asymmetric_eval: 4.94596\n",
      "[450]\ttraining's custom_asymmetric_eval: 3.3298\tvalid_1's custom_asymmetric_eval: 4.94412\n",
      "[500]\ttraining's custom_asymmetric_eval: 3.25879\tvalid_1's custom_asymmetric_eval: 4.93757\n",
      "[550]\ttraining's custom_asymmetric_eval: 3.19497\tvalid_1's custom_asymmetric_eval: 4.92923\n",
      "[600]\ttraining's custom_asymmetric_eval: 3.1357\tvalid_1's custom_asymmetric_eval: 4.92668\n",
      "[650]\ttraining's custom_asymmetric_eval: 3.08203\tvalid_1's custom_asymmetric_eval: 4.92575\n",
      "[700]\ttraining's custom_asymmetric_eval: 3.0311\tvalid_1's custom_asymmetric_eval: 4.92288\n",
      "[750]\ttraining's custom_asymmetric_eval: 2.98503\tvalid_1's custom_asymmetric_eval: 4.92081\n",
      "[800]\ttraining's custom_asymmetric_eval: 2.94013\tvalid_1's custom_asymmetric_eval: 4.91909\n",
      "[850]\ttraining's custom_asymmetric_eval: 2.89849\tvalid_1's custom_asymmetric_eval: 4.91894\n",
      "Early stopping, best iteration is:\n",
      "[804]\ttraining's custom_asymmetric_eval: 2.9369\tvalid_1's custom_asymmetric_eval: 4.91842\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training fold 4\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's custom_asymmetric_eval: 4.6694\tvalid_1's custom_asymmetric_eval: 5.5666\n",
      "[100]\ttraining's custom_asymmetric_eval: 4.18958\tvalid_1's custom_asymmetric_eval: 5.37408\n",
      "[150]\ttraining's custom_asymmetric_eval: 3.94833\tvalid_1's custom_asymmetric_eval: 5.3267\n",
      "[200]\ttraining's custom_asymmetric_eval: 3.78383\tvalid_1's custom_asymmetric_eval: 5.30094\n",
      "[250]\ttraining's custom_asymmetric_eval: 3.65043\tvalid_1's custom_asymmetric_eval: 5.27082\n",
      "[300]\ttraining's custom_asymmetric_eval: 3.54125\tvalid_1's custom_asymmetric_eval: 5.25145\n",
      "[350]\ttraining's custom_asymmetric_eval: 3.44728\tvalid_1's custom_asymmetric_eval: 5.24246\n",
      "[400]\ttraining's custom_asymmetric_eval: 3.36373\tvalid_1's custom_asymmetric_eval: 5.23391\n",
      "[450]\ttraining's custom_asymmetric_eval: 3.29066\tvalid_1's custom_asymmetric_eval: 5.22745\n",
      "[500]\ttraining's custom_asymmetric_eval: 3.2232\tvalid_1's custom_asymmetric_eval: 5.2219\n",
      "[550]\ttraining's custom_asymmetric_eval: 3.16206\tvalid_1's custom_asymmetric_eval: 5.2144\n",
      "[600]\ttraining's custom_asymmetric_eval: 3.1067\tvalid_1's custom_asymmetric_eval: 5.21027\n",
      "[650]\ttraining's custom_asymmetric_eval: 3.05369\tvalid_1's custom_asymmetric_eval: 5.20598\n",
      "[700]\ttraining's custom_asymmetric_eval: 3.00526\tvalid_1's custom_asymmetric_eval: 5.20227\n",
      "[750]\ttraining's custom_asymmetric_eval: 2.95858\tvalid_1's custom_asymmetric_eval: 5.1981\n",
      "[800]\ttraining's custom_asymmetric_eval: 2.9152\tvalid_1's custom_asymmetric_eval: 5.19785\n",
      "[850]\ttraining's custom_asymmetric_eval: 2.87412\tvalid_1's custom_asymmetric_eval: 5.19486\n",
      "[900]\ttraining's custom_asymmetric_eval: 2.83529\tvalid_1's custom_asymmetric_eval: 5.19265\n",
      "[950]\ttraining's custom_asymmetric_eval: 2.79881\tvalid_1's custom_asymmetric_eval: 5.19076\n",
      "[1000]\ttraining's custom_asymmetric_eval: 2.76396\tvalid_1's custom_asymmetric_eval: 5.18824\n",
      "[1050]\ttraining's custom_asymmetric_eval: 2.73159\tvalid_1's custom_asymmetric_eval: 5.18684\n",
      "Early stopping, best iteration is:\n",
      "[1036]\ttraining's custom_asymmetric_eval: 2.74051\tvalid_1's custom_asymmetric_eval: 5.18628\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training fold 5\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's custom_asymmetric_eval: 4.70299\tvalid_1's custom_asymmetric_eval: 5.37967\n",
      "[100]\ttraining's custom_asymmetric_eval: 4.22033\tvalid_1's custom_asymmetric_eval: 5.16273\n",
      "[150]\ttraining's custom_asymmetric_eval: 3.97003\tvalid_1's custom_asymmetric_eval: 5.09119\n",
      "[200]\ttraining's custom_asymmetric_eval: 3.79496\tvalid_1's custom_asymmetric_eval: 5.04791\n",
      "[250]\ttraining's custom_asymmetric_eval: 3.65878\tvalid_1's custom_asymmetric_eval: 5.01666\n",
      "[300]\ttraining's custom_asymmetric_eval: 3.54655\tvalid_1's custom_asymmetric_eval: 4.99495\n",
      "[350]\ttraining's custom_asymmetric_eval: 3.45272\tvalid_1's custom_asymmetric_eval: 4.98393\n",
      "[400]\ttraining's custom_asymmetric_eval: 3.37137\tvalid_1's custom_asymmetric_eval: 4.97303\n",
      "[450]\ttraining's custom_asymmetric_eval: 3.29729\tvalid_1's custom_asymmetric_eval: 4.96561\n",
      "[500]\ttraining's custom_asymmetric_eval: 3.23077\tvalid_1's custom_asymmetric_eval: 4.95737\n",
      "[550]\ttraining's custom_asymmetric_eval: 3.16735\tvalid_1's custom_asymmetric_eval: 4.9527\n",
      "[600]\ttraining's custom_asymmetric_eval: 3.11084\tvalid_1's custom_asymmetric_eval: 4.9479\n",
      "[650]\ttraining's custom_asymmetric_eval: 3.05808\tvalid_1's custom_asymmetric_eval: 4.94258\n",
      "[700]\ttraining's custom_asymmetric_eval: 3.00697\tvalid_1's custom_asymmetric_eval: 4.93833\n",
      "[750]\ttraining's custom_asymmetric_eval: 2.96177\tvalid_1's custom_asymmetric_eval: 4.9359\n",
      "[800]\ttraining's custom_asymmetric_eval: 2.91942\tvalid_1's custom_asymmetric_eval: 4.93286\n",
      "[850]\ttraining's custom_asymmetric_eval: 2.87821\tvalid_1's custom_asymmetric_eval: 4.92958\n",
      "[900]\ttraining's custom_asymmetric_eval: 2.84021\tvalid_1's custom_asymmetric_eval: 4.92832\n",
      "[950]\ttraining's custom_asymmetric_eval: 2.8041\tvalid_1's custom_asymmetric_eval: 4.92518\n",
      "[1000]\ttraining's custom_asymmetric_eval: 2.76864\tvalid_1's custom_asymmetric_eval: 4.92158\n",
      "[1050]\ttraining's custom_asymmetric_eval: 2.735\tvalid_1's custom_asymmetric_eval: 4.92102\n",
      "Early stopping, best iteration is:\n",
      "[1037]\ttraining's custom_asymmetric_eval: 2.74361\tvalid_1's custom_asymmetric_eval: 4.92043\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Our out of folds rmse is 2.1631833975448216\n",
      "Save predictions...\n"
     ]
    }
   ],
   "source": [
    "def train_and_evaluate():\n",
    "    # read data\n",
    "    print('Reading our data...')\n",
    "    data, submission = read_data()\n",
    "    \n",
    "    data['date'] = pd.to_datetime(data['date'])\n",
    "    # get amount of unique days in our data\n",
    "    days = abs((data['date'].min() - data['date'].max()).days)\n",
    "    # how many training data do we need to train with at least 2 years and considering lags\n",
    "    need = 365 + 365 + 90 + 28\n",
    "    print(f'We have {(days - 28)} days of training history')\n",
    "    print(f'we have {(days - 28 - need)} days left')\n",
    "    if (days - 28 - need) > 0:\n",
    "        print('We have enough training data, lets continue')\n",
    "    else:\n",
    "        print('Get more training data, training can fail')\n",
    "    \n",
    "    # simple features\n",
    "    print('Running simple feature engineering...')\n",
    "    data = simple_fe(data)\n",
    "    print('Removing first 118 days')\n",
    "    # eliminate the first 118 days of our train data because of lags\n",
    "    min_date = data['date'].min() + timedelta(days = 118)\n",
    "    data = data[data['date'] > min_date]\n",
    "    \n",
    "    # define our numeric features and categorical features\n",
    "    features = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2',\n",
    "                'snap_CA', 'snap_TX', 'snap_WI', 'sell_price', 'year', 'month', 'quarter', 'week', 'day', 'dayofweek', 'dayofyear',\n",
    "                'demand_rolling_mean_t7', 'demand_rolling_mean_t15', 'demand_rolling_mean_t30', 'demand_rolling_mean_t90',\n",
    "                'demand_rolling_std_t7', 'demand_rolling_std_t15', 'demand_rolling_std_t30', 'demand_rolling_std_t90']\n",
    "    \n",
    "    cat_features = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n",
    "    \n",
    "    print('-'*50)\n",
    "    print('\\n')\n",
    "    print(f'Training model with {len(features)} features...')\n",
    "    # run lgbm model with 5 GroupKFold (subgroups by year, month)\n",
    "    test = run_lgb(data, features, cat_features)\n",
    "    print('Save predictions...')\n",
    "    # predict\n",
    "    predict(test, submission)\n",
    "        \n",
    "# run our program\n",
    "train_and_evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
